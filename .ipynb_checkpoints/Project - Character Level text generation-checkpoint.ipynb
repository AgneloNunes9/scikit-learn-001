{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058cf07e-8092-42b9-ae62-2b20deb37c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b8b189-021b-4577-ad80-283130799210",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"C:/python-ml/datasets/shakesphere\",\"shakesphere.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651834c4-4c72-47fa-bcc9-ef8f16748daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "except FileNotFoundError:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1820901c-9876-45b0-9d93-639d7bc2e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = raw_text.lower()\n",
    "# Transformed the entire corpus to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf69f815-9d1a-4aba-b8c7-898097829a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5447675\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcafbf8d-9fe4-4a7b-ba77-8fd48c156285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary 58\n",
      "\n",
      " !\"&'(),-.0123456789:;<>?[]_`abcdefghijklmnopqrstuvwxyz|}\n"
     ]
    }
   ],
   "source": [
    "# Creating the vocabulary\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_chars = len(chars)\n",
    "\n",
    "print(f\"Total Vocabulary {n_chars}\")\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432c91cc-4d27-40fb-b9ac-3414b780c57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '<': 23, '>': 24, '?': 25, '[': 26, ']': 27, '_': 28, '`': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55, '|': 56, '}': 57}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Vocabulary defines every possible input and output to the model\\n The char_to_int dictionary is crucial for converting the text data into numerical sequences for training.\\n The int_to_char will be used to convert the numbers back to text\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integer Character Mapping\n",
    "char_to_int = {char: i for i, char in enumerate(chars)}\n",
    "# Character Integer Mapping\n",
    "int_to_char = {i : char for i, char in enumerate(chars)}\n",
    "\n",
    "print(f\"Mappings {char_to_int}\")\n",
    "\n",
    "# Why?\n",
    "'''\n",
    " Vocabulary defines every possible input and output to the model\n",
    " The char_to_int dictionary is crucial for converting the text data into numerical sequences for training.\n",
    " The int_to_char will be used to convert the numbers back to text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add57d06-71cf-4933-924e-50932c3f9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the training data\n",
    "\n",
    "# First encoding the entire corpus into numbers\n",
    "\n",
    "transformed_text = [char_to_int[char] for char in raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac3c539-1f7c-4abc-a017-5ec0b524c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Length for the model\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(transformed_text) - seq_len, 1):\n",
    "    # Sequence 0 to 100\n",
    "    seq_in = transformed_text[i: i+seq_len]\n",
    "    # 101th word\n",
    "    seq_out = transformed_text[i+seq_len]\n",
    "\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022565fa-f392-4332-b81e-bab56382d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training patterns (sequences) created: 5447575\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(dataX)\n",
    "print(f\"Total training patterns (sequences) created: {n_patterns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c15824a-a2d6-4bac-a27c-cc5b53801722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataX)\n",
    "Y = np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bea4f52f-f721-4847-a8c9-a29e36016125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted to numpy arrays for efficient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1e4235-40e5-48f7-bf6f-207bd3e0c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def save_batch_to_hdf5(filename, X_batch, y_batch):\n",
    "    \"\"\"Saves a batch of data to a single .h5 file.\"\"\"\n",
    "    with h5py.File(filename, 'w') as hf:\n",
    "        hf.create_dataset('X', data=X_batch)\n",
    "        hf.create_dataset('y', data=y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13d58848-b5c1-45a1-a06e-2bcc44452dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for your processed data\n",
    "directory_path = \"processed_data\"\n",
    "\n",
    "# Create the directory if it doesn't already exist\n",
    "os.makedirs(directory_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e69f01-4fd1-4a76-8647-16a5746c9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFLINE PRE PROCESSING\n",
    "\n",
    "batch_size = 1024\n",
    "num_batches = len(X)// batch_size\n",
    "\n",
    "for i in range(200):\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    X_batch_int = X[start:end]\n",
    "    Y_batch_int = Y[start:end]\n",
    "    X_batch_processed = to_categorical(X_batch_int, num_classes=n_chars)\n",
    "    y_batch_processed = to_categorical(Y_batch_int, num_classes=n_chars)\n",
    "    save_batch_to_hdf5(f'processed_data/batch_{i}.h5', X_batch_processed, y_batch_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef541278-8232-4573-9f85-073695e25112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python-ml\\myvvenv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">80,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">14,906</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m80,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                  │          \u001b[38;5;34m14,906\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,546</span> (373.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m95,546\u001b[0m (373.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,546</span> (373.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m95,546\u001b[0m (373.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the RNN model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 100 sequences, each of one hot vectors\n",
    "model.add(SimpleRNN(256, input_shape=(seq_len, n_chars)))\n",
    "model.add(Dense(n_chars, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584b56d2-b637-4f0f-bae7-eb9698474970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "list_of_files = tf.io.gfile.glob('processed_data/*.h5')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(list_of_files)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=100).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9055a0-abf8-42c3-91af-ab1ad0aed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_file(filepath):\n",
    "    \"\"\"Opens an h5 file and yields its X and y data.\"\"\"\n",
    "    with h5py.File(filepath, 'r') as hf:\n",
    "        X_batch = np.array(hf['X'])\n",
    "        y_batch = np.array(hf['y'])\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424a8e54-b45b-4361-bd53-0a323e2de1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m batch_size = \u001b[32m128\u001b[39m \u001b[38;5;66;03m# The size of the batches you want to feed the model\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Create a dataset of the filenames\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dataset = tf.data.Dataset.from_tensor_slices(\u001b[43mfile_list\u001b[49m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2. For each filename, read the actual data from the file\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#    - from_generator creates a dataset from a Python generator.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#    - It's a clean way to load data that doesn't fit in memory.\u001b[39;00m\n\u001b[32m     10\u001b[39m dataset = dataset.interleave(\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m filepath: tf.data.Dataset.from_generator(\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: read_h5_file(filepath.numpy()),\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     num_parallel_calls=tf.data.AUTOTUNE\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'file_list' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Training Parameters ---\n",
    "batch_size = 128 # The size of the batches you want to feed the model\n",
    "\n",
    "# 1. Create a dataset of the filenames\n",
    "dataset = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "\n",
    "# 2. For each filename, read the actual data from the file\n",
    "#    - from_generator creates a dataset from a Python generator.\n",
    "#    - It's a clean way to load data that doesn't fit in memory.\n",
    "dataset = dataset.interleave(\n",
    "    lambda filepath: tf.data.Dataset.from_generator(\n",
    "        lambda: read_h5_file(filepath.numpy()),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, seq_len, n_chars), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_chars), dtype=tf.float32)\n",
    "        )\n",
    "    ),\n",
    "    cycle_length=tf.data.AUTOTUNE,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Shuffle, Batch, and Prefetch for optimal performance\n",
    "#    - .shuffle() randomizes the order of the sequences to improve learning.\n",
    "#    - .batch() groups the individual sequences into batches.\n",
    "#    - .prefetch() prepares the next batch while the GPU is busy with the current one.\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e369095-153b-4426-9742-7e1c76a9639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking file: processed_data\\batch_0.h5 ---\n",
      "Found 'X' dataset with shape: (1024, 100, 58)\n",
      "Found 'y' dataset with shape: (1024, 58)\n",
      "\n",
      "File appears to be valid.\n",
      "Starting training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attr 'Toutput_types' of 'OptionalFromValue' Op passed list of length 0 less than minimum 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# --- 4. Train the Model ---\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     70\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python-ml\\myvvenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python-ml\\myvvenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:132\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperimental\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    137\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Attr 'Toutput_types' of 'OptionalFromValue' Op passed list of length 0 less than minimum 1."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- 1. Get the List of Files (Same as before) ---\n",
    "file_list = tf.io.gfile.glob('processed_data/*.h5')\n",
    "if not file_list:\n",
    "    print(\"Error: No .h5 files found in 'processed_data'.\")\n",
    "else:\n",
    "    # Pick the first file to inspect\n",
    "    filepath_to_check = file_list[0]\n",
    "    print(f\"--- Checking file: {filepath_to_check} ---\")\n",
    "\n",
    "    try:\n",
    "        with h5py.File(filepath_to_check, 'r') as hf:\n",
    "            # Check if the 'X' and 'y' keys exist\n",
    "            if 'X' not in hf or 'y' not in hf:\n",
    "                print(\"Error: File is missing 'X' or 'y' datasets.\")\n",
    "            else:\n",
    "                X_data = hf['X']\n",
    "                y_data = hf['y']\n",
    "                print(f\"Found 'X' dataset with shape: {X_data.shape}\")\n",
    "                print(f\"Found 'y' dataset with shape: {y_data.shape}\")\n",
    "                print(\"\\nFile appears to be valid.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while reading the file: {e}\")\n",
    "\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# --- 2. Define the Parsing Function with the Correct Wrapper ---\n",
    "\n",
    "# This is the required \"blueprint\" for the function's output\n",
    "output_types = (tf.float32, tf.float32)\n",
    "\n",
    "@tf.py_function(Tout=output_types)\n",
    "def parse_h5_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads the X and y tensors from a single HDF5 file.\n",
    "    The Tout argument above tells TensorFlow what dtypes to expect.\n",
    "    \"\"\"\n",
    "    # filepath is a tf.Tensor, so we need .numpy() to get the string value\n",
    "    with h5py.File(filepath.numpy().decode(), 'r') as hf:\n",
    "        X_batch = np.array(hf['X'], dtype=np.float32)\n",
    "        y_batch = np.array(hf['y'], dtype=np.float32)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "# --- 3. Build the Simplified tf.data Pipeline ---\n",
    "FULL_BATCH_SIZE = 128 # The full batch size you used in pre-processing\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(None, seq_len, n_chars), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, n_chars), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset from our Python generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: h5_file_generator(file_list),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "# Filter out any batches that are not the full size\n",
    "dataset = dataset.filter(lambda x, y: tf.shape(x)[0] == FULL_BATCH_SIZE)\n",
    "\n",
    "# Prefetch the next batch\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# --- 4. Train the Model ---\n",
    "print(\"Starting training...\")\n",
    "model.fit(\n",
    "    dataset,\n",
    "    epochs=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
